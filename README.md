# ESL-L1-question-targeting
GT OMSCS CS6460 Educational Technology Project: ESL Reading Comprehension Question Interface with L1 Targeting. For more context on the purpose and design of this project, see the full proposal here: https://drive.google.com/file/d/1D6FmIAyxu0lAiTcQVQZiAyAmF0SqHFsV/view?usp=sharing

## What is L1 Targeting?
L1 is the term for an English as a Sencond Language (ESL) learner's native language. Research has shown that __% of ESL student error can be traced to L1 influence [cite]. The theory which is the basis of this project is that providing extra practice question resources which specifically target areas of English which are different than a student's L1 can help to prevent these L1-influenced errors and improve the student's ESL learning.

## Project Goal
This project aims to provide a platform for ESL students to receive and answer questions on reading comprehension passages with a filter for the specific questions which would be most useful for this student to study based on their given L1.

## How is L1 Targetting Performed?
Given a set of questions related to an input passage the L1 targetter ranks and re-orders the questions based on a contrastive linguistic score generated by the system. 

This contrastive score is in the range from 0 to 1 and denotes how difficult the question might be for a specific L1 based on the liguistic comparison method of contrastive analysis. In practical terms, this score measures the linguistic variation between the question and surrounding context in English and in the L1 language. This is based on the assumption that questions with higher varience between the English and L1 texts are more difficult for speakers of that L1.

To calculate this score, this system utilizes the Google Translate API to translate the question and surrounding context into the given L1. Both versions of the text have linguistic features extracted using the pre-trained language models from SpaCy and the system uses these to perform the linguistic analyses described below. All of the features are then averaged to gain the total contrastive score for that question.

### Linguistic Features for Contrastive Scoring
- Part of Speech Word Ordering: Compare the part of speech for the words at each position in the sentence between English and L1. This will provide a metric for how the words are reordered during translation.
- Semantic Similarity: Using word embeddings of the sentences in each language, get the semantic similarity metric between the languages. This will measure a general representation on how the word meanings in the sentence fit into their respective languages in context.
- Verb Form Analysis: A simple metric to determine the difference in verb conjugation between languages. This is performed by getting the lemma (word root) of the main verb in the sentence and comparing this difference across languages.
- Dependency Distance: This feature uses the dependency parsing information in the sentence to determine the sentence root, subject, and object. The difference in positions of these words in the sentence are then compared between languages. This will measure the ordering of SVO vs. SOV etc.

## Architecture
Architecture is broken into two components: Web UI front end hosted with Google Cloud Platform (GCP) App Engine, and a backend API hosted with GCP Cloud Run. Both components are written in Python Flask
![Architecture Diagram](architecture-diagram.PNG)

## To deploy the API code in GCP Cloud Run
[![Run on Google Cloud](https://storage.googleapis.com/cloudrun/button.svg)](https://console.cloud.google.com/cloudshell/editor?shellonly=true&cloudshell_image=gcr.io/cloudrun/button&cloudshell_git_repo=https://github.com/byramag/esl-question-generator)

## Standard Question Data
20 sample data files under the path ./data sourced from https://www.myenglishpages.com/site_php_files/reading.php

These reading comprehension passages are relatively short and at a beginner/intermediate reading level and cover a wide range of context domains. The sample passages from the source website contain some human created questions which can be used as the returned questions.

The larger set of data is directed for higher question difficulty and longer passages. This large dataset is the Stanford NLP group's SQuAD 2.0 dataset and can be found here: https://rajpurkar.github.io/SQuAD-explorer/

### Discussion on Question Generation
The question generation functionality was intended to be implemented using a pre-trained model from https://github.com/PaddlePaddle/ERNIE This model was chosen as it is ranked highest in BLEU 4 score on the SQuAD dataset according to https://paperswithcode.com/sota/question-generation-on-squad11. This model is also appropriate as it provides the pre-trained model and the library to interface with it, which will allow for ease of integration.

However, several delays were encountered when trying to implement the use of this and other pretrained models, which led to the decision to reduce scope of this project from dynamic question generation given any input text to the use of large static datasets. Dynamic question generation will remain a future goal to be implemented at a later stage (but not associated with the deliverable for CS6460).

